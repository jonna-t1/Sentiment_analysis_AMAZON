{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c86a4c-5b70-4f1c-9c52-7dbbc3a2f480",
   "metadata": {},
   "outputs": [],
   "source": [
    "## used below df.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a15ca5-ec0c-41f2-a0d8-d60650d6e57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = train_test_split(df, test_size=0.2, random_state=42) #splits dataset 80:20, always produces the isolated test set\n",
    "train_set['reviewText'] = train_set['reviewText'].fillna(\"\")\n",
    "train_set['overall'] = train_set['overall'].fillna(0)\n",
    "test_set['reviewText'] = test_set['reviewText'].fillna(\"\")\n",
    "test_set['overall'] = test_set['overall'].fillna(0)\n",
    "sample_size = utils.sentiCounts(train_set, 'rating')['count'].min() # get minimum count\n",
    "# use min amount to balance the dataset\n",
    "train_set = utils.balanceData(train_set, sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e7b2a3-e998-448a-9a0a-9bf83f3af8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(rating):\n",
    "    if rating <= 2:\n",
    "        # negative\n",
    "        return 0\n",
    "    else:\n",
    "        # positive\n",
    "        return 1\n",
    "train_set['sentiment'] = train_set['overall'].apply(get_sentiment)\n",
    "test_set['sentiment'] = test_set['overall'].apply(get_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517ef68d-d055-4db1-b504-e155de04163d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.to_csv(directory+'test_data.csv', encoding='utf-8', index=False)\n",
    "print(\"Test data uploaded to csv at:\", directory)\n",
    "test_set\n",
    "text_test, y_test = test_set['reviewText'], test_set['sentiment'] # test_set['overall'] provides labels\n",
    "text_train = train_set['reviewText'].fillna(\"\")    # easier naming convention\n",
    "test_set['reviewText']\n",
    "test_set['reviewText'][test_set['reviewText'].isna()].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a03652-b017-450a-9b49-7dc354c6d9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "## using the bag of words model\n",
    "vectorizer = CountVectorizer(max_features=10000)\n",
    "# vectorizer = HashingVectorizer()\n",
    "vect = vectorizer.fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "y_train = train_set['sentiment']\n",
    "print(\"X_train:\\n{}\".format(repr(X_train))) # outputs the matrix representation\n",
    "X_test = vect.transform(text_test)\n",
    "feature_names = vect.get_feature_names_out()\n",
    "print(\"Number of features: {}\".format(len(feature_names)))\n",
    "print(\"First 20 features:\\n{}\".format(feature_names[:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6618f111-6229-49ca-a0f8-2226d85a744e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## this cell isnt needed as i have included it\n",
    "import numpy as np\n",
    "import gensim\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "df['sentiment'] = df['overall'].apply(get_sentiment)\n",
    "# Sample sentences\n",
    "sentences_series = df[\"reviewText\"].fillna(\"\").apply(str).str.lower()\n",
    "\n",
    "# Corresponding labels\n",
    "labels = df['sentiment']\n",
    "\n",
    "tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences_series]\n",
    "# Train Word2Vec\n",
    "word2vec_model = gensim.models.Word2Vec(sentences=tokenized_sentences, vector_size=50, window=5, min_count=1, sg=1)\n",
    "\n",
    "# Function to compute sentence embeddings\n",
    "def sentence_to_vector(sentence, model):\n",
    "    vectors = []\n",
    "    for word in sentence:\n",
    "        if word in model.wv:\n",
    "            vectors.append(model.wv[word])\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "# Generate feature vectors\n",
    "X = np.array([sentence_to_vector(sentence, word2vec_model) for sentence in tokenized_sentences])\n",
    "y = np.array(labels)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Logistic Regression classifier\n",
    "classifier = LogisticRegression(solver='liblinear', max_iter=1000)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
